# Neural Networks are Decision Trees

## Abstract

この原稿では、任意の活性化関数を持つニューラルネットワークが決定木として表現できることを示す。
この表現は近似ではなく等価であるため、ニューラルネットワークの精度を正確に保つことができる。
この研究は、ニューラルネットワークの理解を深め、そのブラックボックス的な性質に取り組む道を開くものであると信じている。
我々はいくつかのニューラルネットワークと等価な決定木を共有し、木表現が解釈可能であることに加えて、小さなネットワークに対して計算上の利点を達成できることを示す。
この解析は、全結合ネットワークと畳み込みネットワークの両方に対して行われ、スキップ接続や正規化も含まれる場合も含まれない場合もある。

## 1. Introduction

過去10年間、ニューラルネットワークは大きな成功を収めてきましたが、その予測のブラックボックス的な性質が、健康やセキュリティなど多くの産業でより広く、より信頼性の高い採用の妨げとなっています。
この事実により、研究者はニューラルネットワークの決定を説明する方法を研究するようになりました。
ニューラルネットワークの決定を説明するための努力は、顕著性マップ、解釈可能な方法による近似、ジョイントモデルなど、いくつかのアプローチに分類することができる。

顕著性マップは、ニューラルネットワークが予測時に最も利用する、入力上の領域を強調する方法である。
以前の研究[20]では、ネットワーク全体の入力特異的な線形化を視覚化するために、入力に対するニューラルネットワークの出力の勾配をとった。
また、別の研究[26]では、決定から特徴に戻すために、decomvnetを使用している。
これらの方法によって得られた顕著性マップは、しばしばノイズが多く、行われた決定の明確な理解を妨げる。
別の手法[29], [18], [4], [6]は、活性化に対するニューラルネットワークの出力の導関数、通常は完全接続層の直前のものを利用するものである。
このトラックや他のいくつかの作品[27]、[11]、[5]によって得られた顕著性マップは、予測されたクラスに関連する領域を強調するという意味で、より明確である。
これらの手法は、判断の支持領域が健全であるかどうかを確認するなどの目的には有用であるが、なぜそのような判断がなされたのかという詳細な論理的理由付けがまだなされていない。

ニューラルネットワークと決定木のような解釈可能なバイデザインモデルとの間の変換は、関心のあるトピックである。
[8]では、決定木でニューラルネットワークを初期化する方法が考案された。
[9, 19, 25] も決定木に相当するニューラルネットを提供している。
これらの作品に登場するニューラルネットワークは特定のアーキテクチャを持つため、その変換はどのようなモデルに対しても一般性を欠く。
[24]では、ニューラルネットワークの決定境界を木で近似するように学習させた。
この研究では、ニューラルネットワークと決定木の対応付けは行っておらず、単に決定木を正則化として用いているに過ぎない。
[7]では、ニューラルネットワークを用いて決定木を学習させた。
このような木の蒸留はニューラルネットワークの近似であり、直接の変換ではないため、ニューラルネットワークが訓練されたタスクでは性能が悪い。

ニューラルネットワークと決定木の共同モデル[12], [16], [13], [14], [17], [2], [10], [22] は一般的に深層学習を使っていくつかの木を支援したり、ニューラルネットワークの構造を考えて、それが木に似ているようにします。
最近の研究[23]では、ニューラルネットワークの最後の完全連結層を決定木に置き換えている。
バックボーンはニューラルネットワークのままなので、意思決定の完全な論理的推論ではなく、人間が意思決定の善し悪しを検証する手段を提供することで説明を実現しようとしている。

本論文では、任意の活性化関数を持つニューラルネットワークが、直接的に等価な決定木表現を持つことを示す。
したがって、誘導される木の出力はニューラルネットワークの出力と全く同じであり、木表現は神経アーキテクチャを何ら制限したり変更したりする必要がない。
この決定木等価性は、ニューラルネットワークの理解を深め、例えば、サンプルが分類されるノードルールによって抽出される、テストサンプルが属するカテゴリを分析することによって、ニューラルネットワークのブラックボックス的な性質に取り組む道を開くと信じています。
我々は、スキップ層や正規化を含む完全連結型または畳み込み型ニューラルネットワークのいずれに対しても、ニューラルネットワークの決定木に相当するものを見出すことができることを示す。
解釈のしやすさという点に加えて、計算量の点でも、記憶容量の増加を犠牲にして、誘導木が対応するニューラルネットワークより有利であることを示す。

この論文を書くにあたり、特にフィードフォワード ReLU ネットワークに関して、私たちの論文と重なる部分があることに気づきました [28], [3], [15], [21] です。
我々はこれらの研究の成果を、任意の活性化関数やリカレントニューラルネットワークに拡張した。

## 2. Decision Tree Analysis of Neural Networks

本節の導出は、まずReLU、Leaky ReLUなどの区分線形活性化関数を持つフィードフォワードニューラルネットワークに対して行う。
次に、任意の活性化関数を持つニューラルネットワークに解析を拡張する。

### 2.1. Fully Connected Networks

ネットワーク $i$ 番目の層の重み行列を $Wi$ とする.
また、$σ$を任意の一次元線形活性化関数、$x0$をニューラルネットワークへの入力とする。
そして、フィードフォワードニューラルネットワークの出力と中間特徴は、式（1）のように表すことができる。

Equation(1).

なお、式1において、最終的な活性化（ソフトマックスなど）は省略し、バイアス項も各$xi$に1を連結することで簡単に含めることができるため、無視している。
活性化関数$σ$は要素ごとのスカラー倍算として働くので、次のように書くことができる。

Equation(2).

式2中、$ai-1$は$WTi-1xi-1$が該当する線形領域の活性度の傾きを示すベクトルで、$\odot$は要素ごとの乗算を表す。
なお、$ai-1$は活性化関数の線形領域の指標（傾き）を含むため、そのままカテゴリ分けの結果と解釈することができる。
式2は以下のように再整理できる。

Equation(3).

式3において、$Wi$に対する列方向要素別乗算として$\odot$を使用する。
これは、$ai-1$の列ベクトルを$Wi$の大きさに合わせて繰り返すことで得られる行列による要素ごとの乗算に相当する。
式3を用いると、式1は以下のように書き換えることができる。

Equation(4).

式4より、入力$x0$に直接適用する層$i$の有効ウエイト行列$WˆTi$を以下のように定義することができる。

Equation(5).

式5において、i層目までの分類ベクトルは以下のように定義される。$ci-1=a0ka1k...ai-1$, ただし$k$は連結演算子。

式5から、層$i$の有効行列は前の層の分類ベクトルにのみ依存することが直接的に観察できる。
これは、各層で、前の分類/決定に基づいて、新しい効率的なフィルタが選択され、ネットワーク入力に適用されることを示している。
このことは、完全連結型ニューラルネットワークが、有効行列が分類ルールとして機能する、単一の決定木として表現できることを直接的に示している。
各層iにおいて、有効行列$Ci-1WˆTi$の応答は$ai$ベクトルに分類され、この分類結果に基づいて、次の層の有効行列$CiWˆTi+1$が決定される。
層 $i$ は $kmi$ 方向の分類として表現される。ここで、$mi$は各層$i$のフィルタの数、$k$は活性化における線形領域の総数である。
このように、ある層$i$における分類は、任意の深さのノードが$k$個の分類に分岐している深さ$mi$の木で表現できる。

アルゴリズム1では、ニューラルネットワークの等価な決定木をよりよく説明するために、ネットワーク全体について、式.5をアルゴリズムとして書き換えている。
ここでは、一般性を損なわず簡単にするために、ReLU活性化関数$a∈{0, 1}$をアルゴリズムに与えている。
アルゴリズム1の5-9行目は決定木のノードに相当し、単純なYES/NOの決定がなされることが明確に観察される。

したがって、ニューラルネットワークに相当する決定木は、アルゴリズム2のように構築することができる。
このアルゴリズムを用いて、3つの層を持つニューラルネットワークに対して得られた木表現を共有する。この場合、層1、2、3はそれぞれ2、1、1のフィルタを持つ。
このネットワークは層間にReLUの活性化を持ち、最後の層以降は活性化しない。
アルゴリズム2と図1から、NN相当木の深さは$d=Pn-2i=0mi$、最後の枝のカテゴリ総数は$2^d$であることがわかる。
一見すると、カテゴリ数は膨大に見える。
例えば、ニューラルネットワークの第一層に64個のフィルターがあるとすると、少なくとも264$個の枝が存在することになり、既に難解である。
しかし、違反や冗長なルールが存在する可能性があり、そのようなルールがあれば、NNに相当する木をロスなく刈り取ることができる。
もう一つは、カテゴリ（木の葉）の数が学習データよりはるかに多いため、学習中にすべてのカテゴリが実現されない可能性が高いことである。
これらのカテゴリはアプリケーションに応じて刈り込むことができ、アプリケーションが許す限り、これらのカテゴリに該当するデータは無効であるとみなすことができる。
次節では、いくつかのニューラルネットワークの決定木を分析することで、このような冗長、違反、未実現のカテゴリが実際に存在することを示す。
しかしその前に、スキップ接続、正規化、畳み込み、他の活性化関数、再帰性に対して、ニューラルネットワークの木に相当するものが存在することを示す。

#### 2.1.1 Skip Connections

以下のようなタイプの残差ニューラルネットワークを解析する。

Equation(6)

式6を用いると、Sec.2.1と同様の解析を経て、${}_{r}{x}_i$を以下のように書き換えることができる。

Equation(7)

最後に、式(7)の $ai-1WˆT i$ を用いて、残差ニューラルネットワークの有効行列を以下のように定義することができる。

Equation(8)

式8から、層$i$について、残差有効行列$rWˆTi$は、以前の活性化からの分類のみに基づいて定義されていることが観察できる。
第2.1節の解析と同様に、これによって残差ニューラルネットワークの木等価が可能になる。

#### 2.1.2 Normalization Layers

正規化層は線形であり、学習後、正規化層の後または前にある線形層に埋め込むことができるため、どの正規化層に対しても個別の解析は必要ない。

### 2.2. Convolutional Neural Networks

ここで、${K}_i : C_{i+1}times C_iTimes M_iTimes N_i $ を入力 ${F}_{i} : C_iTimes H_iTimes W_i$ に適用した層 $i$ のコンボリューションカーネルとする。
ただし、$M_i$と$N_i$はコンボリューションカーネルの空間サイズ、$H_i$と$W_i$は入力の空間サイズを示す。

畳み込みニューラルネットワーク$CNN({F}_0)$の出力と、中間特徴量${F}_{i}$は以下のように書くことができる。

Equation(9)

完全連結ネットワークの解析と同様に、活性化関数の要素ごとのスカラー倍算の性質から、次のように書くことができる。

式10において、ai-1はKiと同じ空間サイズであり、前回の特徴量Fi-1における対応する領域での活性化関数の傾きから構成される。
なお、上記は特定の空間領域に対してのみ成立し、畳み込みKi-1が適用される空間領域ごとに別のai-1が存在する。
例えば、Ki-1が3×3のカーネルである場合、畳み込みが適用される全ての3×3の領域に対して、別々のai-1が存在する。
有効なコンボリューションCi-1Kˆ iは、以下のように書くことができる。

Equation(11)

ただし、式 11 において、Ci-1Kˆ i は、領域ごとに特定の有効畳み込みを含み、領域は、層 i の受容野に従って定義される。

式11から、効果的な畳み込みは活性化から来る分類にのみ依存し、完全連結ネットワークの解析と同様に、木の等価性を可能にすることがわかる。完全連結層の場合との違いは、多くの判断が x0 全体ではなく、部分的な入力領域に対して行われることである。

### 2.3. Continuous Activation Functions

式2において、区分線形活性化では、aの要素は活性化関数の区分線形領域で制限された数の値を持つことができる。
この数は、有効なフィルターあたりの子ノードの数を定義する。
連続活性化関数は、無限の領域を持つ区分線形関数と考えることができるので、連続活性化関数への拡張は些細なことである。
したがって、連続活性化関数の場合、ニューラルネットワークの等価木は、単一のフィルタであっても直ちに無限大の幅を持つようになる。
これは有用な結果ではないかもしれないが、念のためここでこの議論を提供する。
有限の木を保証するために、連続的活性化を量子化したものを使うことも考えられるが、これは少数の区分的線形領域となり、したがって活性化ごとの子ノードも少数となる可能性がある。

### 2.4. Recurrent Networks

リカレントニューラルネットワーク(RNN)はフィードフォワード表現に展開できるため、RNNは決定木と等価に表現することができる。
ここでは、以下のようなリカレントニューラルネットワークを研究する。
なお、バイアス項は入力ベクトルに1を連結することで表現できるため、単純に省略する。

Equation(12)

これまでの解析と同様に、$h(t)$は以下のように書き換えることができる。

Equation(13)

式(13)は次のように書き換えることができる.

Equation(14)

ただし、式(14)において、積演算子は行列の乗算を表し、そのステップは-1であり、i=tのとき積演算子の出力を1と考える。
cjWˆ jを導入することで、式(14)を以下のように書き換えることができる。

Equation(15)

式(15)と式(12)を組み合わせると、$o(t)$は以下のように書くことができる。

Equation(16)

式(16)はさらに次のように簡略化できる.

Equation(17)

式(17)において、ciZˆ T i = a (t)Vˆ T ciWˆ i となる。
式(17)からわかるように、RNNの出力は、分類ベクトルciにのみ依存するため、これまでの解析と同様に、木の等価性が可能となる。

RNNの場合、式12におけるσの一般的な選択はtanhであることに注意されたい。
セクション2.3で述べたように、有限木を提供するために、tanhの部分的な線形近似を使用することを考慮することができる。

## 3. Experimental Results

まず、おもちゃの実験で、y = x 2 の式にニューラルネットワークを当てはめる。
このニューラルネットワークは2つのフィルタを持つ3つの密な層から構成されている。
このネットワークは完全連結層の後にリーク・リリュー活性化を用いているが、最後の層は後活性化をしていない。
リーキー-ReLUには、Tensorflowのデフォルト値である0.3の負のスロープを使用した[1]。
ネットワークは5000組の(x, y)で学習され、xは[-2.5, 2.5]区間から規則的にサンプリングされた。
図2は、ニューラルネットワークに対応する決定木を示している。
決定木において、黒い四角のボックスはルールを示し、ボックスから左の子はルールが成立しないことを、右の子はルールが成立することを意味する。
よりよく可視化するために、ルールは w T x + β > 0 を x に作用する直接不等式に変換することによって得られる。
これは，x がスカラーであるため，特定の回帰 y = x 2 に対して行うことができる．
すべての葉において、ネットワークはこれまでの決定に基づいて線形関数（赤い長方形で示される）を適用する。
スペースが限られているため、これらの関数を明示的に書くことは避けた。
一見すると、この例のニューラルネットワークの木表現は、2 Pn-2 i mi = 24 = 16の分類があるため大きく見える。
しかし、決定木の中の多くのルールは冗長であり、したがって決定木の中のいくつかのパスは無効であることに気がつく。
冗長なルールの例として、x < -1.16ルールが成立した後にx < 0.32をチェックすることが挙げられる。
これは、直接的にこのノードの無効な左子ノードを作成する。
したがって、この場合、左の子を削除し、分類ルールをより厳しいもの、つまり x < -1.16 にマージすることで、木をきれいにすることができる。
Fig.2の決定木をクリーニングすると、Fig.3aのようなシンプルな木になり、16のカテゴリーではなく、5つのカテゴリーになる。
この5つのカテゴリは、Fig.3bのモデル応答からも直接見ることができる。
このように、ニューラルネットワークの解釈は簡単で、決定木表現によって境界が決定された各領域について、ネットワークは非線形方程式y = x 2を線形方程式で近似しているのである。
決定木の解釈とその推論を明確にすることができるが、その一部を以下に示す。
ニューラルネットワークは回帰問題の対称性を把握することができず、それは決定境界が非対称であることからも明らかである。
1.16以下と1以上の領域は境界がないため、xがこれらの境界を超えるとニューラルデシジョンは精度を失う。

次に、半月形の分類という別のおもちゃの問題を調べ、ニューラルネットワークによって生成される決定木を分析する。
リーキー・ReLU活性を持つ3つの層からなる完全連結型ニューラルネットワークを学習する。
各層は2つのフィルタを持っているが、最後の層は1つである。
学習されたネットワークによって生成されたクリーンな決定木はFig.4である。
決定木は多くのカテゴリを発見し、その境界は木の中のルールによって決定される。
図5では、カテゴリを見やすくするために、カテゴリを異なる色で表示している。
決定木からいくつかの推論ができる。例えば、いくつかの領域は非常によく定義され、境界があり、その分類は学習データと完全に一致しているため、これらの領域は非常に信頼性が高い。
正確な分類境界を得るのに役立つが、訓練データのコンパクトな表現を提供することができない非拘束カテゴリがあり、これらはニューラル決定によって行われた不正確な外挿に対応する可能性がある。
また、学習データが一つも該当しないにもかかわらず、出現したカテゴリーもある。

解釈のしやすさという側面以外に、決定木表現は計算上でもいくつかの利点をもたらす。
 表1では、ニューラルネットワークとそれによって誘導された木のパラメータ数、浮動小数点数の比較、乗算・加算の演算を比較している。
なお、カテゴリごとに木の深さが異なるため、木表現における比較、乗算、加算は期待値で与えている。
誘導された木はニューラルネットワークの展開であるため、すべての可能な経路をカバーし、有効なフィルタをすべて記憶している。
したがって、予想通り、ニューラルネットワークの木表現におけるパラメータ数は、ネットワークのそれよりも多くなる。
誘導木では、各層iにおいて、最大mi個のフィルターが入力に直接適用されるが、ニューラルネットワークでは常にmi個のフィルターが前の特徴に適用され、これは通常特徴次元において入力よりはるかに大きい。
このように、計算の面では、ニューラルネットワークに比べて木表現が有利である。

## 4. Conclusion

この原稿では、ニューラルネットワークが決定木と等価に表現できることを示した。
この木の等価性は、完全接続層、畳み込み層、残差接続、正規化、リカレント層、および任意の活性化に対して成り立つ。
この木の等価性は、ニューラルネットワークのブラックボックス的な性質に取り組むための方向性を提供すると考えている。

## LICENCE

Article by Aytekin, Caglar. “Neural Networks Are Decision Trees.” arXiv, October 25, 2022. [https://doi.org/10.48550/arXiv.2210.05189.
](https://doi.org/10.48550/arXiv.2210.05189) / [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) / Translated.
